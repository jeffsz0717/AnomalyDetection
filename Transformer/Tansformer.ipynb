{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, N, d_model):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.N = N\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Ws = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "        self.Q = self.K = self.V = self.sigma = torch.zeros((N, d_model))\n",
    "\n",
    "        self.P = torch.zeros((N, N))\n",
    "        self.S = torch.zeros((N, N))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.initialize(x)\n",
    "        self.P = self.prior_association()\n",
    "        self.S = self.series_association()\n",
    "        Z = self.reconstruction()\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def initialize(self, x):\n",
    "        self.Q = self.Wq(x)\n",
    "        self.K = self.Wk(x)\n",
    "        self.V = self.Wv(x)\n",
    "        self.sigma = self.Ws(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_kernel(mean, sigma):\n",
    "        normalize = 1 / (math.sqrt(2 * torch.pi) * sigma)\n",
    "        return normalize * torch.exp(-0.5 * (mean / sigma).pow(2))\n",
    "\n",
    "    def prior_association(self):\n",
    "        p = torch.from_numpy(\n",
    "            np.abs(np.indices((self.N, self.N))[0] - np.indices((self.N, self.N))[1])\n",
    "        )\n",
    "        gaussian = self.gaussian_kernel(p.float(), self.sigma)\n",
    "        gaussian /= gaussian.sum(dim=-1).view(-1, 1)\n",
    "\n",
    "        return gaussian\n",
    "\n",
    "    def series_association(self):\n",
    "        return F.softmax((self.Q @ self.K.T) / math.sqrt(self.d_model), dim=0)\n",
    "\n",
    "    def reconstruction(self):\n",
    "        return self.S @ self.V\n",
    "\n",
    "\n",
    "class AnomalyTransformerBlock(nn.Module):\n",
    "    def __init__(self, N, d_model):\n",
    "        super().__init__()\n",
    "        self.N, self.d_model = N, d_model\n",
    "\n",
    "        self.attention = AnomalyAttention(self.N, self.d_model)\n",
    "        self.ln1 = nn.LayerNorm(self.d_model)\n",
    "        self.ff = nn.Sequential(nn.Linear(self.d_model, self.d_model), nn.ReLU())\n",
    "        self.ln2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_identity = x\n",
    "        x = self.attention(x)\n",
    "        z = self.ln1(x + x_identity)\n",
    "\n",
    "        z_identity = z\n",
    "        z = self.ff(z)\n",
    "        z = self.ln2(z + z_identity)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, N, d_model, layers, lambda_):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [AnomalyTransformerBlock(self.N, self.d_model) for _ in range(layers)]\n",
    "        )\n",
    "        self.output = None\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        self.P_layers = []\n",
    "        self.S_layers = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            self.P_layers.append(block.attention.P)\n",
    "            self.S_layers.append(block.attention.S)\n",
    "\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def layer_association_discrepancy(self, Pl, Sl, x):\n",
    "        rowwise_kl = lambda row: (\n",
    "            F.kl_div(Pl[row, :], Sl[row, :]) + F.kl_div(Sl[row, :], Pl[row, :])\n",
    "        )\n",
    "        ad_vector = torch.concat(\n",
    "            [rowwise_kl(row).unsqueeze(0) for row in range(Pl.shape[0])]\n",
    "        )\n",
    "        return ad_vector\n",
    "\n",
    "    def association_discrepancy(self, P_list, S_list, x):\n",
    "\n",
    "        return (1 / len(P_list)) * sum(\n",
    "            [\n",
    "                self.layer_association_discrepancy(P, S, x)\n",
    "                for P, S in zip(P_list, S_list)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def loss_function(self, x_hat, P_list, S_list, lambda_, x):\n",
    "        frob_norm = torch.linalg.matrix_norm(x_hat - x, ord=\"fro\")\n",
    "        return frob_norm - (\n",
    "            lambda_\n",
    "            * torch.linalg.norm(self.association_discrepancy(P_list, S_list, x), ord=1)\n",
    "        )\n",
    "\n",
    "    def min_loss(self, x):\n",
    "        P_list = self.P_layers\n",
    "        S_list = [S.detach() for S in self.S_layers]\n",
    "        lambda_ = -self.lambda_\n",
    "        return self.loss_function(self.output, P_list, S_list, lambda_, x)\n",
    "\n",
    "    def max_loss(self, x):\n",
    "        P_list = [P.detach() for P in self.P_layers]\n",
    "        S_list = self.S_layers\n",
    "        lambda_ = self.lambda_\n",
    "        return self.loss_function(self.output, P_list, S_list, lambda_, x)\n",
    "\n",
    "    def anomaly_score(self, x):\n",
    "        ad = F.softmax(\n",
    "            -self.association_discrepancy(self.P_layers, self.S_layers, x), dim=0\n",
    "        )\n",
    "\n",
    "        assert ad.shape[0] == self.N\n",
    "\n",
    "        norm = torch.tensor(\n",
    "            [\n",
    "                torch.linalg.norm(x[i, :] - self.output[i, :], ord=2)\n",
    "                for i in range(self.N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        assert norm.shape[0] == self.N\n",
    "\n",
    "        score = torch.mul(ad, norm)\n",
    "\n",
    "        return score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
